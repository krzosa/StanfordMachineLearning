{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks overview\n",
    "\n",
    "\n",
    "**Training**\n",
    "1. Randomly initialize the weights\n",
    "1. Implement forward propagation to get $h\\theta(x(i))$ for any $x(i)x^{(i)}x(i)$\n",
    "1. Implement the cost function\n",
    "1. Implement backpropagation to compute partial derivatives\n",
    "1. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "1. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.\n",
    "\n",
    "**Calculating output - Forward Propagation(week4)**\\\n",
    "Let's say we have a network like this: \n",
    "\n",
    "$$\\begin{bmatrix} x0 \\newline x1 \\newline x2 \\newline x3 \\end{bmatrix} -> \\begin{bmatrix}a1 \\newline a2 \\newline a3 \\end{bmatrix} -> h\\theta(x)$$\n",
    "\n",
    "Then we can calculate our hypothesis function like this:\n",
    "\n",
    "$$\\begin{align*} a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) \\newline a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) \\newline a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) \\newline h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)}) \\newline \\end{align*}$$\n",
    "\n",
    "Every node of layer n is connected to every node(except bias) of layer n-1. All of them have their own parameters theta. Activation values of the nodes of the first layer are equal to the input variables. Using those we can caluculate activation values of the next layer. activation value n-1 * corresponding theta n-1. We take that value and input it into g() function which is sigmoid function which outputs the final activation value of layer n. We do the same thing for remaining layers including final layer which returns our hypothesis.\n",
    "\n",
    "From this we can conclude that our theta is a matrix with dimensions nrOfnodes(n+1) X nrOfNodes(n)+1## not sure if this is correct - numberOfNodes(n) X numberOfNodes(n-1) + 1##. Where n = layer and where +1 basicaly means bias node\n",
    "\n",
    "**Vectorized implementation**\\\n",
    "$z(j)=\\theta(j−1)a(j−1)$\\\n",
    "$(j)=g(z(j))$ g is sigmoid func applied element-wise\n",
    "\n",
    "**Multiclass classification**\\\n",
    "To classify data into multiple classes/categories we let our hypothesis function return a vector o values. For example let's say we have four classes. Neural network should output a vector with 4 values out of which 3 should equal to \"0\" and the most likely class to be correct should equal \"1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost function**\n",
    "* L - total number of layers\n",
    "* $s_l$ - number of units/nodes (not counting bias) in layer l\n",
    "* K - number of output classes\n",
    "\n",
    "$$\\begin{gather*} J(\\Theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[y^{(i)}_k \\log ((h_\\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\\log (1 - (h_\\Theta(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} ( \\Theta_{j,i}^{(l)})^2\\end{gather*}$$\n",
    "\n",
    "First summation loops through all the training examples (m), second summation loops through all the possible output classes (k). Regularization part accounts for all the theta matrices in neural network. l means layer in third summation. Two remaining summations loop through theta matrix.\n",
    "\n",
    "Double sum simply adds up the logistic regression costs calculated for each cell in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation Algorithm**\n",
    "\n",
    "Goal = $minimizeJ(\\theta)$\n",
    "\n",
    "Given training set $\\lbrace (x^{(1)}, y^{(1)}) \\cdots (x^{(m)}, y^{(m)})\\rbrace$\n",
    "* Set $\\Delta^{(l)}_{ij}$ := 0 (matrix full of zeros)\n",
    "\n",
    "For training example t=1 to m:\n",
    "1. Set $a^{(1)} := x^{(t)}$\n",
    "1. Perform forward propagation to compute $a^{(l)}$ for $l=2,3...L$\n",
    "1. Using $y^{t}$, compute $\\delta^{(L)} = a{(L)} - y{(L)}$\\\n",
    "Where L = number of layers(last layer). So our \"error values\" for the last layer are simply the differences between predicted values and actual values. \n",
    "1. Compute $\\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)})$\\\n",
    "Delta values are partial derivatives of the cost function  $\\frac{\\delta}{\\delta z_j^{(l)}}cost(i)$\\\n",
    "To compute delta values of remaining layers we can use equation that steps us back from right to left.\\\n",
    ".* = elements wise multiplication\\\n",
    "g' = derivative of activation function evaluated with with the input $z^{(l)}$\\\n",
    "$g'(z^{(l)}) = a^{(l)}\\ .*\\ (1 - a^{(l)})$\n",
    "1. We update the$\\Delta$ matrix with:\\\n",
    "$\\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)}+a_j^{(l)}\\delta_i^{(l+1)}$\\\n",
    "or with vectorization:\\\n",
    "$\\Delta^{(l)} := \\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T$\n",
    "\n",
    "with which we can update capital-delta matrix D which is used as an \"accumulator\" to add up our values as we go along and eventually compute our partial derivative. Thus we get $\\frac \\partial {\\partial \\Theta_{ij}^{(l)}} J(\\Theta)$\n",
    "* $D^{(l)}_{i,j} := \\dfrac{1}{m}\\left(\\Delta^{(l)}_{i,j} + \\lambda\\Theta^{(l)}_{i,j}\\right)$ if j!=0\\\n",
    "* $D^{(l)}_{i,j} := \\dfrac{1}{m}\\Delta^{(l)}_{i,j}$ if j=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Unrolling parameters\n",
    "A = ones(10,6);\n",
    "Asize = size(A,1)*size(A,2);\n",
    "B = ones(10,11)*2;\n",
    "Bsize = size(B,1)*size(B,2);\n",
    "C = ones(10,11)*3;\n",
    "Csize = size(C,1)*size(C,2);\n",
    "D= [A(:); B(:); C(:)];\n",
    "reshape(D(1:Asize), size(A,1), size(A,2));\n",
    "reshape(D(Asize+1:Asize+Bsize), size(B,1), size(B,2));\n",
    "reshape(D(Asize+Bsize+1:Asize+Bsize+Csize), size(C,1), size(C,2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient checking**\\\n",
    "Is used to test if backpropagation works as intended:\n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial\\Theta_j}J(\\Theta) \\approx \\dfrac{J(\\Theta_1, \\dots, \\Theta_j + \\epsilon, \\dots, \\Theta_n) - J(\\Theta_1, \\dots, \\Theta_j - \\epsilon, \\dots, \\Theta_n)}{2\\epsilon}$$\n",
    "\n",
    "Small value for epsilon such as $\\epsilon = 10^{-4}$ guarantees that the math works out properly. If the value for epsilon is too small we can end up with computation problems.\n",
    "\n",
    "```Octave\n",
    "epsilon = 1e-4;\n",
    "for i = 1:n,\n",
    "  thetaPlus = theta;\n",
    "  thetaPlus(i) += epsilon;\n",
    "  thetaMinus = theta;\n",
    "  thetaMinus(i) -= epsilon;\n",
    "  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)\n",
    "end;\n",
    "```\n",
    "\n",
    "We only need to verify once that backpropagation works. This is a very slow algorithm so we should remove it once we verify that everything is ok ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We should initialize theta with random values...*\\\n",
    "between <-epsilon, epsilon> (unrelated to epsilon above).\n",
    "\n",
    "example:\n",
    "```Octave\n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
